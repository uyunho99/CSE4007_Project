{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.83M/2.83M [00:01<00:00, 1.53MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.00k/1.00k [00:00<00:00, 635kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", \n",
    "                                                    bos_token='<s>', # begin of sentence token\n",
    "                                                    eos_token='</s>', # end of sentence token\n",
    "                                                    unk_token='<unk>', # unknown token\n",
    "                                                    pad_token='<pad>', # pad token(문장의 길이를 맞추기 위해 사용)\n",
    "                                                    mask_token='<mask>') # mask token(마스킹을 통해 어떤 단어를 예측할지 결정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁안녕', '하', '세', '요.', '▁반', '갑', '습니다.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"안녕하세요. 반갑습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['eos_token_ids'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m안녕하세요. 반갑습니다. 저는\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m gen_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids,\n\u001b[1;32m      8\u001b[0m                          max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m                          repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m                          pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     11\u001b[0m                          eos_token_ids\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     12\u001b[0m                          bos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     13\u001b[0m                          use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     15\u001b[0m generated \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(gen_ids[\u001b[39m0\u001b[39m,:]\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(generated)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.8/site-packages/transformers/generation/utils.py:1213\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[0;32m-> 1213\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m   1215\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m logits_processor \u001b[39m=\u001b[39m logits_processor \u001b[39mif\u001b[39;00m logits_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.8/site-packages/transformers/generation/utils.py:1105\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1106\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1108\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['eos_token_ids'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "text = \"안녕하세요. 반갑습니다. 저는\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=100,\n",
    "                         repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_ids=tokenizer.eos_token_id,\n",
    "                         bos_token_id=tokenizer.bos_token_id,\n",
    "                         use_cache=True)\n",
    "\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use kogpt based GPT-3 later.\n",
    "\n",
    "https://huggingface.co/kakaobrain/kogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 252/252 [00:00<00:00, 49.3kB/s]\n",
      "Downloading (…)oat16/tokenizer.json: 100%|██████████| 2.51M/2.51M [00:01<00:00, 1.66MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 88.0/88.0 [00:00<00:00, 57.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  'kakaobrain/kogpt', \n",
    "  revision = 'KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  bos_token = '[BOS]', # begin of sentence token\n",
    "  eos_token = '[EOS]', # end of sentence token\n",
    "  unk_token = '[UNK]', # unknown token\n",
    "  pad_token = '[PAD]', # 패드 토큰: 패딩(전체 문장 길이를 맞추기 위해 빈 공간을 채우는 것)을 위해 사용\n",
    "  mask_token = '[MASK]' # 마스킹 토큰: 마스킹을 통해 언어 모델이 어떤 단어를 예측해야 하는지 알려줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'kakaobrain/kogpt',\n",
    "    revision = 'KoGPT6B-ryan1.5b-float16',\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "    torch_dtype = 'auto',\n",
    "    low_cpu_mem_usage = True).to(device = 'cuda', non_blocking = True)\n",
    "\n",
    "_ = model.eval()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
