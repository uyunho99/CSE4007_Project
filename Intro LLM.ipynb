{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLM(GPT-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages, define model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:06<00:00, 171kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:01<00:00, 424kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 131kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 548M/548M [00:32<00:00, 16.9MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 26.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog and runs off.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says\n"
     ]
    }
   ],
   "source": [
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "\"What's wrong with you?\"\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"What's wrong with you?\"\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"What's wrong\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "\"What's wrong with you?\" he asks. \"You're not going to be able to do anything about it. You're just a fox. I don't know what you're doing\n"
     ]
    }
   ],
   "source": [
    "beam_outputs2 = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, # 다섯 개의 문장을 리턴\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_outputs2[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog along the pilot's body and... is \"tree legs forth the bunny.\" Echo, the new co-worker, attempts to break she so cagey. Then it's a doozy when Lumpy and\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, # 샘플링 사용\n",
    "    max_length=50, \n",
    "    top_k=0 #top_k=0으로 설정하면 타임스텝별로 하나의 토큰만 샘플링\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-p Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog, who slithers along in the dog's lap, as the dog bites the red fox into the ground, and then he goes over to the dog's rear. The dog bites the front paw into\n"
     ]
    }
   ],
   "source": [
    "topp_sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, #샘플링 전략 사용\n",
    "    max_length=50, # 최대 디코딩 길이는 50\n",
    "    top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "    top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "    num_return_sequences=3 #3개의 결과를 디코딩해낸다\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(topp_sample_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 3.38k/3.38k [00:00<00:00, 1.13MB/s]\n",
      "Downloading readme: 100%|██████████| 3.96k/3.96k [00:00<00:00, 1.18MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset klaid/ljp to /Users/yunho/.cache/huggingface/datasets/lawcompany___klaid/ljp/1.0.0/170cd5bb1a0d9f3e383773bc69b51eb6a717918f91b682fe094492d865feaf4c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 137M/137M [00:15<00:00, 8.93MB/s] \n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset klaid downloaded and prepared to /Users/yunho/.cache/huggingface/datasets/lawcompany___klaid/ljp/1.0.0/170cd5bb1a0d9f3e383773bc69b51eb6a717918f91b682fe094492d865feaf4c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lawcompany/KLAID\", 'ljp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['laws_service_id', 'fact', 'laws_service'],\n",
       "        num_rows: 161192\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laws_service_id</th>\n",
       "      <th>fact</th>\n",
       "      <th>laws_service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>피고인은 2018. 8. 9. 23:33경 술을 마신 상태로 경산시 사동에 있는 상...</td>\n",
       "      <td>도로교통법 제148조의2 제2항,도로교통법 제44조 제2항</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>피고인은 2016. 3. 19. 10:16경 경북 칠곡군 왜관읍 왜관대교 앞 도로에...</td>\n",
       "      <td>도로교통법 제152조 제1호,도로교통법 제43조</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>피고인은 2016. 10. 10 16:55경 평택시 오성면 복합화력발전소 앞 도로에...</td>\n",
       "      <td>도로교통법 제152조 제1호,도로교통법 제43조</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>피고인 A은 노동일에 종사 중이다. 피고인은 2017. 2. 2. 20:00경 부산...</td>\n",
       "      <td>형법 제260조 제1항</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>피고인은 2015. 7. 2. 06:35경 부산 부산진구 B에 있는 C슈퍼 앞길에서...</td>\n",
       "      <td>형법 제314조 제1항</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161187</th>\n",
       "      <td>46</td>\n",
       "      <td>피고인은 구리시 C에 있는 D부동산을 운영하면서 ‘E상가조합’을 결성하여 위 상가조...</td>\n",
       "      <td>형법 제356조,형법 제355조 제1항</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161188</th>\n",
       "      <td>34</td>\n",
       "      <td>피고인은 2015. 4. 16. 15:16경 대구 달서구 C에 있는 D 주유소에서,...</td>\n",
       "      <td>형법 제260조 제1항</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161189</th>\n",
       "      <td>8</td>\n",
       "      <td>피고인은 2020. 7. 18. 06:20경 수원시 영통구 B에 있는 ‘C’ 식당 ...</td>\n",
       "      <td>도로교통법 제148조의2 제1항,도로교통법 제44조 제1항</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161190</th>\n",
       "      <td>1</td>\n",
       "      <td>피고인은 2011. 3. 15. 18:30경 서울 서초구 B에 있는 전 배우자인 C...</td>\n",
       "      <td>형법 제136조 제1항</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161191</th>\n",
       "      <td>111</td>\n",
       "      <td>1. 공무집행방해 피고인은 2018. 9. 22. 04:25경 울산 남구 B에 있는...</td>\n",
       "      <td>형법 제136조 제1항,형법 제311조</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        laws_service_id                                               fact  \\\n",
       "0                    32  피고인은 2018. 8. 9. 23:33경 술을 마신 상태로 경산시 사동에 있는 상...   \n",
       "1                     0  피고인은 2016. 3. 19. 10:16경 경북 칠곡군 왜관읍 왜관대교 앞 도로에...   \n",
       "2                     0  피고인은 2016. 10. 10 16:55경 평택시 오성면 복합화력발전소 앞 도로에...   \n",
       "3                    34  피고인 A은 노동일에 종사 중이다. 피고인은 2017. 2. 2. 20:00경 부산...   \n",
       "4                    11  피고인은 2015. 7. 2. 06:35경 부산 부산진구 B에 있는 C슈퍼 앞길에서...   \n",
       "...                 ...                                                ...   \n",
       "161187               46  피고인은 구리시 C에 있는 D부동산을 운영하면서 ‘E상가조합’을 결성하여 위 상가조...   \n",
       "161188               34  피고인은 2015. 4. 16. 15:16경 대구 달서구 C에 있는 D 주유소에서,...   \n",
       "161189                8  피고인은 2020. 7. 18. 06:20경 수원시 영통구 B에 있는 ‘C’ 식당 ...   \n",
       "161190                1  피고인은 2011. 3. 15. 18:30경 서울 서초구 B에 있는 전 배우자인 C...   \n",
       "161191              111  1. 공무집행방해 피고인은 2018. 9. 22. 04:25경 울산 남구 B에 있는...   \n",
       "\n",
       "                            laws_service  \n",
       "0       도로교통법 제148조의2 제2항,도로교통법 제44조 제2항  \n",
       "1             도로교통법 제152조 제1호,도로교통법 제43조  \n",
       "2             도로교통법 제152조 제1호,도로교통법 제43조  \n",
       "3                           형법 제260조 제1항  \n",
       "4                           형법 제314조 제1항  \n",
       "...                                  ...  \n",
       "161187             형법 제356조,형법 제355조 제1항  \n",
       "161188                      형법 제260조 제1항  \n",
       "161189  도로교통법 제148조의2 제1항,도로교통법 제44조 제1항  \n",
       "161190                      형법 제136조 제1항  \n",
       "161191             형법 제136조 제1항,형법 제311조  \n",
       "\n",
       "[161192 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, TextClassificationPipeline, Trainer, TrainingArguments\n",
    "\n",
    "# Load the pre-trained GPT-3.5 model and tokenizer\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "\n",
    "# Load the legal text classification dataset\n",
    "df = pd.read_csv(\"legal_dataset.csv\")\n",
    "\n",
    "# Convert the text labels to numerical values\n",
    "label2id = {\"employment law\": 0, \"contract law\": 1, \"intellectual property law\": 2}\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: label2id[x])\n",
    "\n",
    "# Tokenize the input text and encode the labels\n",
    "inputs = tokenizer(df[\"text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Instantiate the Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=inputs,\n",
    "    train_labels=labels,\n",
    "    data_collator=lambda data: {\"input_ids\": torch.stack([x[\"input_ids\"] for x in data]),\n",
    "                                \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in data]),\n",
    "                                \"labels\": torch.tensor([x[\"labels\"] for x in data])},\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Use the trained model to make predictions on new text\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "text = \"This is a legal document related to a contract dispute.\"\n",
    "prediction = pipeline(text)[0]\n",
    "predicted_label = list(label2id.keys())[list(label2id.values()).index(prediction[\"label\"])]\n",
    "print(\"Predicted label:\", predicted_label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "def analyze_legal_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=text,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text\n",
    "\n",
    "legal_text = \"The defendant has been found guilty of the charge of theft. He is hereby sentenced to 2 years in prison and a fine of $10,000.\"\n",
    "\n",
    "analysis = analyze_legal_text(legal_text)\n",
    "print(analysis)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Load the legal text data\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='train.txt',\n",
    "    block_size=128\n",
    ")\n",
    "eval_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='eval.txt',\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='test.txt',\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "trainer.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
