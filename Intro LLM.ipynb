{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLM(GPT-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages, define model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog and runs off.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says\n"
     ]
    }
   ],
   "source": [
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "\"What's wrong with you?\"\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"What's wrong with you?\"\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"What's wrong\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "\"What's wrong with you?\" he asks. \"You're not going to be able to do anything about it. You're just a fox. I don't know what you're doing\n"
     ]
    }
   ],
   "source": [
    "beam_outputs2 = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, # 다섯 개의 문장을 리턴\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_outputs2[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog by the grass and gets away without a second thought.\n",
      "\n",
      "But the guy is adamant about turning him into something nice instead. I'm hoping he doesn't discard it, especially before the front half of\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, # 샘플링 사용\n",
    "    max_length=50, \n",
    "    top_k=0 #top_k=0으로 설정하면 타임스텝별로 하나의 토큰만 샘플링\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-p Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The quick brown fox jumps over the lazy dog to get at the back of his head, then it jumps off to try to get at his throat, and the lazy dog gives a little squeak before hopping down. It tries to get at its throat\n"
     ]
    }
   ],
   "source": [
    "topp_sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, #샘플링 전략 사용\n",
    "    max_length=50, # 최대 디코딩 길이는 50\n",
    "    top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "    top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "    num_return_sequences=3 #3개의 결과를 디코딩해낸다\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(topp_sample_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
